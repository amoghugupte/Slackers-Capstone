{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"top\"></a>\n",
    "## Assignment 2\n",
    "\n",
    "In this assignment, you should start doing EDA and you need to do at least the following:\n",
    "1. understand the meanings of each feature\n",
    "2. understand the data types of each feature (when Pandas loads your data, the auto detection of data types may not always be correct)\n",
    "3. plot features to better understand the basic descriptive statistics, distribution, correlation, etc. \n",
    "4. spot potential data issues: missing data, outliers, data errors, etc. \n",
    "\n",
    "You should have a Markdown cell on top of the notebook to BRIEFLY summarize the findings/insights from EDA and how EDA helps to guide the following steps (data preprocessing, model building, etc.)\n",
    "\n",
    "You need to submit a notebook file **AND** the PDF export of the notebook (easier for me to view directly on Canvas) to Canvas - to be clear: I am expecting two files (.ipynb and .pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 1. understand the meanings of each feature\n",
    "#### 2. understand the data types of each feature (when Pandas loads your data, the auto detection of data types may not always be correct)\n",
    "<br>\n",
    "Please see the below table for meaning and data-type of each feature:\n",
    "\n",
    "| Column Name | Description | Data type |\n",
    "| --- | --- | --- |\n",
    "| AMT_ANNUITY | Loan annuity | Number |\n",
    "| AMT_CREDIT | Credit amount of the loan | Number |\n",
    "| AMT_GOODS_PRICE | For consumer loans it is the price of the goods for which the loan is given | Number |\n",
    "| AMT_INCOME_TOTAL | Income of the client | Number |\n",
    "| AMT_REQ_CREDIT_BUREAU_DAY | Number of enquiries to Credit Bureau about the client one day before application (excluding one hour before application) | Number |\n",
    "| AMT_REQ_CREDIT_BUREAU_HOUR | Number of enquiries to Credit Bureau about the client one hour before application | Number |\n",
    "| AMT_REQ_CREDIT_BUREAU_MON | Number of enquiries to Credit Bureau about the client one month before application (excluding one week before application) | Number |\n",
    "| AMT_REQ_CREDIT_BUREAU_QRT | Number of enquiries to Credit Bureau about the client 3 month before application (excluding one month before application) | Number |\n",
    "| AMT_REQ_CREDIT_BUREAU_WEEK | Number of enquiries to Credit Bureau about the client one week before application (excluding one day before application) | Number |\n",
    "| AMT_REQ_CREDIT_BUREAU_YEAR | Number of enquiries to Credit Bureau about the client one day year (excluding last 3 months before application) | Number |\n",
    "| CNT_CHILDREN | Number of children the client has | Number |\n",
    "| CNT_FAM_MEMBERS | How many family members does client have | Number |\n",
    "| CODE_GENDER | Gender of the client | Text |\n",
    "| DAYS_BIRTH | Client's age in days at the time of application | Number |\n",
    "| DAYS_EMPLOYED | How many days before the application the person started current employment | Number |\n",
    "| DAYS_ID_PUBLISH | How many days before the application did client change the identity document with which he applied for the loan | Number |\n",
    "| DAYS_LAST_PHONE_CHANGE | How many days before application did client change phone | Number |\n",
    "| DAYS_REGISTRATION | How many days before the application did client change his registration | Number |\n",
    "| DEF_30_CNT_SOCIAL_CIRCLE | How many observation of client's social surroundings defaulted on 30 DPD (days past due)  | Number |\n",
    "| DEF_60_CNT_SOCIAL_CIRCLE | How many observation of client's social surroundings defaulted on 60 (days past due) DPD | Number |\n",
    "| EXT_SOURCE_2 | Normalized score from external data source | Number |\n",
    "| EXT_SOURCE_3 | Normalized score from external data source | Number |\n",
    "| FLAG_CONT_MOBILE | Was mobile phone reachable (1=YES, 0=NO) | Number |\n",
    "| FLAG_DOCUMENT_10 | Did client provide document 10 | Number |\n",
    "| FLAG_DOCUMENT_11 | Did client provide document 11 | Number |\n",
    "| FLAG_DOCUMENT_12 | Did client provide document 12 | Number |\n",
    "| FLAG_DOCUMENT_13 | Did client provide document 13 | Number |\n",
    "| FLAG_DOCUMENT_14 | Did client provide document 14 | Number |\n",
    "| FLAG_DOCUMENT_15 | Did client provide document 15 | Number |\n",
    "| FLAG_DOCUMENT_16 | Did client provide document 16 | Number |\n",
    "| FLAG_DOCUMENT_17 | Did client provide document 17 | Number |\n",
    "| FLAG_DOCUMENT_18 | Did client provide document 18 | Number |\n",
    "| FLAG_DOCUMENT_19 | Did client provide document 19 | Number |\n",
    "| FLAG_DOCUMENT_2 | Did client provide document 2 | Number |\n",
    "| FLAG_DOCUMENT_20 | Did client provide document 20 | Number |\n",
    "| FLAG_DOCUMENT_21 | Did client provide document 21 | Number |\n",
    "| FLAG_DOCUMENT_3 | Did client provide document 3 | Number |\n",
    "| FLAG_DOCUMENT_4 | Did client provide document 4 | Number |\n",
    "| FLAG_DOCUMENT_5 | Did client provide document 5 | Number |\n",
    "| FLAG_DOCUMENT_6 | Did client provide document 6 | Number |\n",
    "| FLAG_DOCUMENT_7 | Did client provide document 7 | Number |\n",
    "| FLAG_DOCUMENT_8 | Did client provide document 8 | Number |\n",
    "| FLAG_DOCUMENT_9 | Did client provide document 9 | Number |\n",
    "| FLAG_EMAIL | Did client provide email (1=YES, 0=NO) | Number |\n",
    "| FLAG_EMP_PHONE | Did client provide work phone (1=YES, 0=NO) | Number |\n",
    "| FLAG_MOBIL | Did client provide mobile phone (1=YES, 0=NO) | Number |\n",
    "| FLAG_OWN_CAR | Flag if the client owns a car | Text |\n",
    "| FLAG_OWN_REALTY | Flag if client owns a house or flat | Text |\n",
    "| FLAG_PHONE | Did client provide home phone (1=YES, 0=NO) | Number |\n",
    "| FLAG_WORK_PHONE | Did client provide home phone (1=YES, 0=NO) | Number |\n",
    "| HOUR_APPR_PROCESS_START | Approximately at what hour did the client apply for the loan | Number |\n",
    "| LIVE_CITY_NOT_WORK_CITY | Flag if client's contact address does not match work address (1=different, 0=same, at city level) | Number |\n",
    "| LIVE_REGION_NOT_WORK_REGION | Flag if client's contact address does not match work address (1=different, 0=same, at region level) | Number |\n",
    "| NAME_CONTRACT_TYPE | Identification if loan is cash or revolving | Text |\n",
    "| NAME_EDUCATION_TYPE | Level of highest education the client achieved | Text |\n",
    "| NAME_FAMILY_STATUS | Family status of the client | Text |\n",
    "| NAME_HOUSING_TYPE | What is the housing situation of the client (renting, living with parents, ...) | Text |\n",
    "| NAME_INCOME_TYPE | Clients income type (businessman, working, maternity leave,â€¦) | Text |\n",
    "| NAME_TYPE_SUITE | Who was accompanying client when he was applying for the loan | Text |\n",
    "| OBS_30_CNT_SOCIAL_CIRCLE | How many observation of client's social surroundings with observable 30 DPD (days past due) default | Number |\n",
    "| OBS_60_CNT_SOCIAL_CIRCLE | How many observation of client's social surroundings with observable 60 DPD (days past due) default | Number |\n",
    "| OCCUPATION_TYPE | What kind of occupation does the client have | Text |\n",
    "| ORGANIZATION_TYPE | Type of organization where client works | Text |\n",
    "| REG_CITY_NOT_LIVE_CITY | Flag if client's permanent address does not match contact address (1=different, 0=same, at city level) | Number |\n",
    "| REG_CITY_NOT_WORK_CITY | Flag if client's permanent address does not match work address (1=different, 0=same, at city level) | Number |\n",
    "| REG_REGION_NOT_LIVE_REGION | Flag if client's permanent address does not match contact address (1=different, 0=same, at region level) | Number |\n",
    "| REG_REGION_NOT_WORK_REGION | Flag if client's permanent address does not match work address (1=different, 0=same, at region level) | Number |\n",
    "| REGION_POPULATION_RELATIVE | Normalized population of region where client lives (higher number means the client lives in more populated region) | Number |\n",
    "| REGION_RATING_CLIENT | Our rating of the region where client lives (1,2,3) | Number |\n",
    "| REGION_RATING_CLIENT_W_CITY | Our rating of the region where client lives with taking city into account (1,2,3) | Number |\n",
    "| SK_ID_CURR | ID of loan in our sample | Number |\n",
    "| WEEKDAY_APPR_PROCESS_START | On which day of the week did the client apply for the loan | Text |\n",
    "\n",
    "\n",
    "#### 3. plot features to better understand the basic descriptive statistics, distribution, correlation, etc.\n",
    "[EDA](#eda)\n",
    "\n",
    "#### 4. spot potential data issues: missing data, outliers, data errors, etc.\n",
    "Please see the below for EDA analysis:\n",
    "1. Some data for EXT_SOURCE_1, EXT_SOURCE_2, EXT_SOURCE_3, AMT_REQ_CREDIT_BUREAU_QRT missing, we may have to delete these rows and see if we can create the model, worst case we may have to remove these columns and check if the model can be generated.\n",
    "1. Mean income is 168,798. Max income is 117,000,000\n",
    "1. Mean credit is 599,026. Max credit is 4,050,000\n",
    "1. Mean age is 44 years, Min age is 21 years, Max age is 69 years\n",
    "1. Mean employment tenure is skewed because of few outliers, which we will filter to get a better model.\n",
    "1. AMT_REQ_CREDIT_BUREAU_QRT - Number of enquiries to Credit Bureau about the client 3 month before application (excluding one month before\n",
    "application)\n",
    "1. FLAG_OWN_CAR, FLAG_OWN_REALTY valid values are Y and N\n",
    "1. NAME_INCOME_TYPE valid values are - Bussinessman, Commercial associate, Maternity leave, Pensioner, State servant, Student, unemployed, Working\n",
    "1. NAME_EDUCATION_TYPE valid values  are - Academic degree, Higher education, Incomplete higher, Lower secondary, Secondary/secondary special\n",
    "1. NAME_HOUSING_TYPE valid values are - Co-op apartment, House / apartment, Municipal apartment, Office apartment, Rented apartment, With parents\n",
    "1. REGION_RATING_CLIENT, REGION_RATING_CLIENT_W_CITY  valid values are - 1, 2, 3\n",
    "1. ORGANIZATION_TYPE has too many categories may cause over fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1\n",
    "\n",
    "## Summary\n",
    "This data set is uploaded in order to get the insights of Credit card Defaultees based on the respective attributes. \n",
    "The consumer lending line of business @ JPMC is in the business of lending money to customers for loans, credit cards, mortgage, etc. and a model like this which can predict potential defaults would be immensely helpful in making lending decisions. The aim of the project is to provide this service, based on a ML model, which will be repeatable, scallable and retrainable.\n",
    "\n",
    "## Data Set\n",
    "Data set is from Kaggle : https://www.kaggle.com/mishra5001/credit-card\n",
    "\n",
    "Source:\n",
    "\n",
    "International Institute of Information Technology Bangalore\n",
    "\n",
    "The Data was collected as part of Social Experiment to provide public inferences of how a person applying for Loan can get it completed in a minimal amount of time. Also, adhering to the facts as which type of customers fail to repay the installments or full loan and provide inference so that the person applying for loan does not falls into that category.\n",
    "\n",
    "### Description from Kaggle:\n",
    "#### Motive!\n",
    "\n",
    "This data set is uploaded in order to get the insights of Credit card Defaultees based on the respective attributes!\n",
    "\n",
    "#### Inside?\n",
    "\n",
    "We have attributes such as IncomeTotal,AMTAPPLICATION,AMT_CREDIT and around 122 Columns in Application Data Set. The interesting thing is if you intend to see the patterns and variations, we can use the PREVIOUS APPLICATION data set also, in order to get more insights.!\n",
    "\n",
    "#### Inspiration\n",
    "We took this data set as our assignment and tried to perform the EDA to the best of our capability!\n",
    "\n",
    "## Solution approach\n",
    "This is a supervised classification problem since we are using existing features and target variable to first train the model on a subset of data and then evaluate the model's performance on the test data. We would pick the model that best meets the criteria of accuracy and performance. We are trying to predict if a loan request from a customer will default.\n",
    "\n",
    "\n",
    "## Value to JP Morgan\n",
    "The consumer lending line of business @ JPMC is in the business of lending money to customers for loans, credit cards, mortgage, etc. and a model like this which can predict potential defaults would be immensely helpful in making lending decisions, Also help JPMC to identify and train their customers for credit discipline.\n",
    "\n",
    "\n",
    "\n",
    "## Aim of the project\n",
    "Use the data to come up with a classification model, to approve or reject the application. Eventually build a scalable, repeatable service to give live classification decisions.\n",
    "\n",
    "## Plan for deliverables\n",
    "All code and analysis will be checkin in github:\n",
    "https://github.com/amoghugupte/Slackers-Capstone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # main focus of this Notebook!\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as pyplot\n",
    "import datetime\n",
    "\n",
    "sns.set()\n",
    "\n",
    "# df is a common name for a dataframe\n",
    "df = pd.read_csv('application_data.csv') # read into a pandas DataFrame (df)\n",
    "\n",
    "# head() is a handy way to visualize what you've loaded. \n",
    "df.head() \n",
    "\n",
    "#pd.set_option('display.height', 1000)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "model_fit_cores = 16\n",
    "model_verbose_level = 4\n",
    "#data_size_limit = 167732"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dataframe with a subset of columns\n",
    "\n",
    "\n",
    "## The below column list gives 167K odd non null rows, hence keeping this for EDA\n",
    "cols = [\"SK_ID_CURR\", \"TARGET\", \"NAME_CONTRACT_TYPE\", \"CODE_GENDER\", \"FLAG_OWN_CAR\", \"FLAG_OWN_REALTY\", \"CNT_CHILDREN\", \"AMT_INCOME_TOTAL\", \"AMT_CREDIT\", \"NAME_INCOME_TYPE\", \"NAME_EDUCATION_TYPE\", \"NAME_FAMILY_STATUS\", \"NAME_HOUSING_TYPE\", \"REGION_POPULATION_RELATIVE\", \"DAYS_BIRTH\", \"DAYS_EMPLOYED\", \"DAYS_REGISTRATION\", \"DAYS_ID_PUBLISH\", \"FLAG_MOBIL\", \"FLAG_EMP_PHONE\", \"FLAG_WORK_PHONE\", \"FLAG_CONT_MOBILE\", \"FLAG_PHONE\", \"FLAG_EMAIL\", \"REGION_RATING_CLIENT\", \"REGION_RATING_CLIENT_W_CITY\", \"WEEKDAY_APPR_PROCESS_START\", \"HOUR_APPR_PROCESS_START\", \"REG_REGION_NOT_LIVE_REGION\", \"REG_REGION_NOT_WORK_REGION\", \"LIVE_REGION_NOT_WORK_REGION\", \"REG_CITY_NOT_LIVE_CITY\", \"REG_CITY_NOT_WORK_CITY\", \"LIVE_CITY_NOT_WORK_CITY\", \"ORGANIZATION_TYPE\", \"FLAG_DOCUMENT_2\", \"FLAG_DOCUMENT_3\", \"FLAG_DOCUMENT_4\", \"FLAG_DOCUMENT_5\", \"FLAG_DOCUMENT_6\", \"FLAG_DOCUMENT_7\", \"FLAG_DOCUMENT_8\", \"FLAG_DOCUMENT_9\", \"FLAG_DOCUMENT_10\", \"FLAG_DOCUMENT_11\", \"FLAG_DOCUMENT_12\", \"FLAG_DOCUMENT_13\", \"FLAG_DOCUMENT_14\", \"FLAG_DOCUMENT_15\", \"FLAG_DOCUMENT_16\", \"FLAG_DOCUMENT_17\", \"FLAG_DOCUMENT_18\", \"FLAG_DOCUMENT_19\", \"FLAG_DOCUMENT_20\", \"FLAG_DOCUMENT_21\", \"DAYS_LAST_PHONE_CHANGE\", \"CNT_FAM_MEMBERS\", \"AMT_ANNUITY\", \"AMT_GOODS_PRICE\", \"OBS_30_CNT_SOCIAL_CIRCLE\", \"DEF_30_CNT_SOCIAL_CIRCLE\", \"OBS_60_CNT_SOCIAL_CIRCLE\", \"DEF_60_CNT_SOCIAL_CIRCLE\", \"NAME_TYPE_SUITE\", \"AMT_REQ_CREDIT_BUREAU_HOUR\", \"AMT_REQ_CREDIT_BUREAU_DAY\", \"AMT_REQ_CREDIT_BUREAU_WEEK\", \"AMT_REQ_CREDIT_BUREAU_MON\", \"AMT_REQ_CREDIT_BUREAU_QRT\", \"AMT_REQ_CREDIT_BUREAU_YEAR\", \"OCCUPATION_TYPE\"]\n",
    "\n",
    "\n",
    "#'SK_ID_PREV','AMT_APPLICATION', 'NAME_CONTRACT_STATUS', 'DAYS_DECISION', 'CODE_REJECT_REASON', 'NAME_CLIENT_TYPE'  - No data\n",
    "#'EXT_SOURCE_1',  'EXT_SOURCE_2', 'EXT_SOURCE_3', \n",
    "#'OCCUPATION_TYPE',\n",
    "temp1 = df.loc[:,cols].copy() # all rows (:)\n",
    "temp1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1.describe(include='all').transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1 = temp1.dropna()\n",
    "temp1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"sp\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA <a class=\"anchor\" id=\"eda\"></a>\n",
    "\n",
    "[top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1.TARGET.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1- 14207/(153525 + 14207)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1.FLAG_OWN_CAR.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1.FLAG_OWN_REALTY.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_type_counts = temp1.NAME_INCOME_TYPE.value_counts()\n",
    "\n",
    "print (income_type_counts)\n",
    "income_type_counts.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "education_type_counts = temp1.NAME_EDUCATION_TYPE.value_counts()\n",
    "sns.set(rc={'figure.figsize':(15,10)})\n",
    "print (education_type_counts)\n",
    "\n",
    "education_type_counts.plot(kind='pie').set(title='Education Type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_type_count = temp1.NAME_HOUSING_TYPE.value_counts()\n",
    "\n",
    "print (housing_type_count)\n",
    "housing_type_count.plot(kind='pie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1.OCCUPATION_TYPE.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1.REGION_RATING_CLIENT.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1.REGION_RATING_CLIENT_W_CITY.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1.ORGANIZATION_TYPE.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(y=\"AMT_INCOME_TOTAL\", x=\"TARGET\", data=temp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cols = [\"NAME_CONTRACT_TYPE\", \"CODE_GENDER\", \"FLAG_OWN_CAR\", \"FLAG_OWN_REALTY\", \"NAME_INCOME_TYPE\", \"NAME_EDUCATION_TYPE\", \"NAME_FAMILY_STATUS\", \"NAME_HOUSING_TYPE\", \"WEEKDAY_APPR_PROCESS_START\", \"ORGANIZATION_TYPE\", \"NAME_TYPE_SUITE\", \"OCCUPATION_TYPE\"]\n",
    "\n",
    "for col in text_cols:\n",
    "    print (\"*******************************************************************************************\")\n",
    "    print (col)\n",
    "    print (temp1[col].value_counts())\n",
    "    print (\"*******************************************************************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = [\"SK_ID_CURR\", \"CNT_CHILDREN\", \"AMT_INCOME_TOTAL\", \"AMT_CREDIT\", \"REGION_POPULATION_RELATIVE\", \"DAYS_BIRTH\", \"DAYS_EMPLOYED\", \"DAYS_REGISTRATION\", \"DAYS_ID_PUBLISH\", \"FLAG_MOBIL\", \"FLAG_EMP_PHONE\", \"FLAG_WORK_PHONE\", \"FLAG_CONT_MOBILE\", \"FLAG_PHONE\", \"FLAG_EMAIL\", \"REGION_RATING_CLIENT\", \"REGION_RATING_CLIENT_W_CITY\", \"HOUR_APPR_PROCESS_START\", \"REG_REGION_NOT_LIVE_REGION\", \"REG_REGION_NOT_WORK_REGION\", \"LIVE_REGION_NOT_WORK_REGION\", \"REG_CITY_NOT_LIVE_CITY\", \"REG_CITY_NOT_WORK_CITY\", \"LIVE_CITY_NOT_WORK_CITY\", \"FLAG_DOCUMENT_2\", \"FLAG_DOCUMENT_3\", \"FLAG_DOCUMENT_4\", \"FLAG_DOCUMENT_5\", \"FLAG_DOCUMENT_6\", \"FLAG_DOCUMENT_7\", \"FLAG_DOCUMENT_8\", \"FLAG_DOCUMENT_9\", \"FLAG_DOCUMENT_10\", \"FLAG_DOCUMENT_11\", \"FLAG_DOCUMENT_12\", \"FLAG_DOCUMENT_13\", \"FLAG_DOCUMENT_14\", \"FLAG_DOCUMENT_15\", \"FLAG_DOCUMENT_16\", \"FLAG_DOCUMENT_17\", \"FLAG_DOCUMENT_18\", \"FLAG_DOCUMENT_19\", \"FLAG_DOCUMENT_20\", \"FLAG_DOCUMENT_21\", \"DAYS_LAST_PHONE_CHANGE\", \"CNT_FAM_MEMBERS\", \"AMT_ANNUITY\", \"AMT_GOODS_PRICE\", \"OBS_30_CNT_SOCIAL_CIRCLE\", \"DEF_30_CNT_SOCIAL_CIRCLE\", \"OBS_60_CNT_SOCIAL_CIRCLE\", \"DEF_60_CNT_SOCIAL_CIRCLE\", \"AMT_REQ_CREDIT_BUREAU_HOUR\", \"AMT_REQ_CREDIT_BUREAU_DAY\", \"AMT_REQ_CREDIT_BUREAU_WEEK\", \"AMT_REQ_CREDIT_BUREAU_MON\", \"AMT_REQ_CREDIT_BUREAU_QRT\", \"AMT_REQ_CREDIT_BUREAU_YEAR\"]\n",
    "cat_features = [\"NAME_CONTRACT_TYPE\", \"CODE_GENDER\", \"FLAG_OWN_CAR\", \"FLAG_OWN_REALTY\", \"NAME_INCOME_TYPE\", \"NAME_FAMILY_STATUS\", \"NAME_EDUCATION_TYPE\", \"NAME_HOUSING_TYPE\", \"WEEKDAY_APPR_PROCESS_START\", \"ORGANIZATION_TYPE\", \"NAME_TYPE_SUITE\", \"OCCUPATION_TYPE\"]\n",
    "#\"NAME_FAMILY_STATUS\", "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# Create the preprocessing pipeline for numerical features\n",
    "# There are two steps in this pipeline\n",
    "# Pipeline(steps=[(name1, transform1), (name2, transform2), ...]) \n",
    "# NOTE the step names can be arbitrary\n",
    "\n",
    "# Step 1 is what we discussed before - filling the missing values if any using mean\n",
    "# Step 2 is feature scaling via standardization - making features look like normal-distributed \n",
    "# see sandardization: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n",
    "num_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        ('num_imputer', SimpleImputer()),  # we will tune differet strategies later\n",
    "        ('scaler', StandardScaler()),\n",
    "        ]\n",
    ")\n",
    "\n",
    "# Create the preprocessing pipelines for the categorical features\n",
    "# There are two steps in this pipeline:\n",
    "# Step 1: filling the missing values if any using the most frequent value\n",
    "# Step 2: one hot encoding\n",
    "\n",
    "cat_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        ('cat_imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore')),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Assign features to the pipelines and Combine two pipelines to form the preprocessor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num_pipeline', num_pipeline, num_features),\n",
    "        ('cat_pipeline', cat_pipeline, cat_features),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "pipeline_dt = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('clf_dt', DecisionTreeClassifier()),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# set up the values of hyperparameters you want to evaluate\n",
    "# here you must use the step names as the prefix followed by two under_scores to sepecify the parameter names and the \"full path\" of the steps\n",
    "\n",
    "# we are trying 2 different impputer strategies \n",
    "# 2x5 different decision tree models with different parameters\n",
    "# in total we are trying 2x2x5 = 20 different combinations\n",
    "\n",
    "param_grid_dt = [\n",
    "    {\n",
    "        'preprocessor__num_pipeline__num_imputer__strategy': ['mean', 'median'],\n",
    "        'clf_dt__criterion': ['gini', 'entropy'], \n",
    "        'clf_dt__max_depth': [3, 4, 5, 6, 7],\n",
    "    }\n",
    "]\n",
    "\n",
    "# set up the grid search \n",
    "grid_search_dt = GridSearchCV(pipeline_dt, param_grid_dt, cv=10, scoring='accuracy', n_jobs=model_fit_cores, verbose=model_verbose_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = temp1.drop(['TARGET'], axis=1)\n",
    "y = temp1['TARGET']\n",
    "\n",
    "print (X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine them back for resampling\n",
    "train_data = pd.concat([X_train, y_train], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate minority and majority classes\n",
    "negative = train_data[train_data.TARGET==0]\n",
    "positive = train_data[train_data.TARGET==1]\n",
    "# upsample minority\n",
    "pos_upsampled = resample(positive,\n",
    "    replace=True, # sample with replacement\n",
    "    n_samples=len(negative), # match number in majority class\n",
    "    random_state=27) # reproducible results\n",
    "# combine majority and upsampled minority\n",
    "upsampled = pd.concat([negative, pos_upsampled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check new class counts\n",
    "upsampled.TARGET.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = upsampled.drop(['TARGET'], axis=1)\n",
    "y_train = upsampled['TARGET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model using the full pipeline\n",
    "grid_search_dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the best performing parameter combination\n",
    "grid_search_dt.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build-in CV results keys\n",
    "sorted(grid_search_dt.cv_results_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test score for the 20 decision tree models\n",
    "grid_search_dt.cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best decistion tree model test score\n",
    "grid_search_dt.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try random forest classifer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# rf pipeline\n",
    "pipeline_rf = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('clf_rf', RandomForestClassifier()),\n",
    "])\n",
    "\n",
    "# here we are trying 2x3 different rf models\n",
    "param_grid_rf = [\n",
    "    {\n",
    "        'clf_rf__criterion': ['gini', 'entropy'], \n",
    "        'clf_rf__n_estimators': [50, 100, 150],  \n",
    "    }\n",
    "]\n",
    "\n",
    "# set up the grid search \n",
    "grid_search_rf = GridSearchCV(pipeline_rf, param_grid_rf, cv=10, scoring='accuracy', n_jobs=model_fit_cores, verbose=model_verbose_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# train the model using the full pipeline\n",
    "grid_search_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the best performing parameter combination\n",
    "grid_search_rf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build-in CV results keys\n",
    "sorted(grid_search_rf.cv_results_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test score for the 20 decision tree models\n",
    "grid_search_rf.cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best decistion tree model test score\n",
    "grid_search_rf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best test score\n",
    "print('best dt score is: ', grid_search_dt.best_score_)\n",
    "print('best rf score is: ', grid_search_rf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the best model\n",
    "# the best parameters are shown, note SimpleImputer() implies that mean strategry is used\n",
    "clf_best = grid_search_rf.best_estimator_\n",
    "clf_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final test on the testing set\n",
    "# To predict on new data: simply calling the predict method \n",
    "# the full pipeline steps will be applied to the testing set followed by the prediction\n",
    "y_pred = clf_best.predict(X_test)\n",
    "\n",
    "# calculate accuracy, precision, recall, f1-score\n",
    "# Note: y_test is the ground truth for the tesing set\n",
    "# we have similiar score for the testing set as the cross validation score - good\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "print(f'Accuracy Score : {accuracy_score(y_test, y_pred)}')\n",
    "print(f'Precision Score : {precision_score(y_test, y_pred)}')\n",
    "print(f'Recall Score : {recall_score(y_test, y_pred)}')\n",
    "print(f'F1 Score : {f1_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "print (y_test.value_counts())\n",
    "\n",
    "(unique, counts) = numpy.unique(y_pred, return_counts=True)\n",
    "frequencies = numpy.asarray((unique, counts)).T\n",
    "print(frequencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_norm = temp1['TARGET'].value_counts(normalize=True)\n",
    "group_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_pred = np.zeros(y_test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our model is better than the base line - good\n",
    "print(f'Baseline Accuracy Score : {accuracy_score(y_test, baseline_pred)}')\n",
    "print(f'Our Best Accuracy Score : {accuracy_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_best.named_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_best.named_steps['preprocessor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = clf_best['clf_rf'].feature_importances_\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_best['preprocessor'].transformers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_best[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_best[0].transformers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_original_feature_names = clf_best[0].transformers_[0][2]\n",
    "num_original_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_original_feature_names = clf_best[0].transformers_[1][2]\n",
    "cat_original_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_new_feature_names = list(clf_best[0].transformers_[1][1]['onehot'].get_feature_names(cat_original_feature_names))\n",
    "cat_new_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = num_original_feature_names + cat_new_feature_names\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = pd.DataFrame(i, index=feature_names, columns=['importance'])\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.sort_values('importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
